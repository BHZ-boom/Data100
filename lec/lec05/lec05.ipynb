{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5 â€“ Data 100, Spring 2022\n",
    "\n",
    "Notebook by Lisa Yan<br/>\n",
    "Content by Lisa Yan, Joseph Gonzalez, Deborah Nolan, Sam Lau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:15.363920Z",
     "start_time": "2018-02-02T15:15:14.337886Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "\n",
    "sns.set()\n",
    "sns.set_context('talk')\n",
    "np.set_printoptions(threshold=20, precision=2, suppress=True)\n",
    "pd.set_option('display.max_rows', 7)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "pd.set_option('precision', 2)\n",
    "# This option stops scientific notation for pandas\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure: File Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducible Data Science\n",
    "\n",
    "In the interest of **reproducible data science** we will download the data programatically.  We have defined some helper functions in the [ds100_utils.py](ds100_utils.py) file.  I can then reuse these helper functions in many different notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:15.368501Z",
     "start_time": "2018-02-02T15:15:15.365808Z"
    }
   },
   "outputs": [],
   "source": [
    "from ds100_utils import fetch_and_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Occasionally, you will want to modify code that you have imported.  To reimport those modifications you can either use the python importlib library:\n",
    "\n",
    "```python\n",
    "from importlib import reload\n",
    "reload(utils)\n",
    "```\n",
    "\n",
    "or use iPython magic which will intelligently import code when files change:\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Downloading the Data\n",
    "\n",
    "Notice that because I record how I got the data in the notebook, others can reproduce this experiment.  However, it is worth noting that **the data can change**.  We will want to pay attention to file timestamps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) CSV \n",
    "Restaurant food safety scores for restaurants in San Francisco. The scores and violation information have been made available by the [San Francisco Department of Public Health](https://data.sfgov.org/Health-and-Social-Services/Restaurant-Scores-LIVES-Standard/pyih-qa8i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:16.391752Z",
     "start_time": "2018-02-02T15:15:15.413412Z"
    }
   },
   "outputs": [],
   "source": [
    "restaurants_file = fetch_and_cache(\n",
    "    \"https://data.sfgov.org/api/views/pyih-qa8i/rows.csv\", # DL from this website\n",
    "    \"restaurants.csv\",         # save as this file\n",
    "    force=False)               # do nothing if the file already exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we even begin to load the data it often helps to understand a little about the high-level structure:\n",
    "\n",
    "1. How big is the data file?\n",
    "1. How is the data file formatted?\n",
    "1. How do we read the data into pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How big is the data?\n",
    "\n",
    "I often like to start my analysis by getting a rough estimate of the size of the data.  This will help inform the tools I use and how I view the data.  If it is relatively small I might use a text editor or a spreadsheet to look at the data.  If it is larger, I might jump to more programmatic exploration or even used distributed computing tools.\n",
    "\n",
    "However here we will use python tools to probe the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(restaurants_file, \"is\", os.path.getsize(restaurants_file) / 1e6, \"MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these seem to be text files I might also want to investigate the number of lines, which often corresponds to the number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:18.993584Z",
     "start_time": "2018-02-02T15:15:18.989848Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(restaurants_file, \"r\") as f:\n",
    "    print(restaurants_file, \"is\", sum(1 for l in f), \"lines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. How is the data file formatted?\n",
    "\n",
    "Let's assume that these are text files (and do not contain binary encoded data) so we can print a \"few lines\" to get a better understanding of the file.\n",
    "\n",
    "Notice that below I use the `repr` function to return the raw string with special characters.  This is helpful in deducing the file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(restaurants_file, \"======================\")\n",
    "with open(restaurants_file, \"r\") as f:\n",
    "    for i in range(10):\n",
    "        print(f\"{i:03} | {repr(f.readline())}\") # zero pad line numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How do we read the data into pandas?\n",
    "\n",
    "With CSVs, we can use the handy `pd.read_csv` [function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html?highlight=read_csv):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = pd.read_csv(restaurants_file)\n",
    "restaurants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side Note: TSV\n",
    "\n",
    "We won't go through all the steps for TSV, but here's how the same SF restaurants data would look like in a TSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_tsv_file = \"data/restaurants.tsv\"  # I processed this earlier\n",
    "\n",
    "print(restaurants_tsv_file, \"======================\")\n",
    "with open(restaurants_tsv_file, \"r\") as f:\n",
    "    for i in range(10):\n",
    "        print(f\"{i:03} | {repr(f.readline())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pd.read_csv` function also reads in TSVs if we specify the `'\\t'` delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants_tsv = pd.read_csv(restaurants_tsv_file, delimiter='\\t')\n",
    "restaurants_tsv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "Note how records with special punctuation (e.g., a comma or apostrophe) are stored in the text file, and how `pd.read_csv` parses them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation in a CSV record entry\n",
    "with open(restaurants_file, \"r\") as f:\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        if \"Mo'z Cafe,\" in line:\n",
    "            print(f\"{i:03} | {repr(line)}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_csv is smart!\n",
    "restaurants[restaurants['business_name'].str.contains(\"Mo'z\")].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "## 2) JSON\n",
    "The City of Berkeley Open Data [website](https://data.cityofberkeley.info/Health/COVID-19-Confirmed-Cases/xn6j-b766) has a dataset with COVID-19 Confirmed Cases among Berkeley residents by date.\n",
    "\n",
    "Let's download this file, saving it as a JSON (note the source URL file type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_file = fetch_and_cache(\n",
    "    \"https://data.cityofberkeley.info/api/views/xn6j-b766/rows.json?accessType=DOWNLOAD\",\n",
    "    \"confirmed-cases.json\",\n",
    "    force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. How big is the data file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(covid_file, \"is\", os.path.getsize(covid_file) / 1e6, \"MB\")\n",
    "\n",
    "with open(covid_file, \"r\") as f:\n",
    "    print(covid_file, \"is\", sum(1 for l in f), \"lines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "### 2. How is the data file formatted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw text\n",
    "print(covid_file, \"======================\")\n",
    "with open(covid_file, \"r\") as f:\n",
    "    for i in range(20):\n",
    "        print(f\"{i:03} | {repr(f.readline())}\") # zero pad line numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "This appears to be a fairly standard JSON file.  We notice that the file appears to contain a description of itself in a field called \"meta\" (which is presumably short for **metadata**).\n",
    "\n",
    "<br/><br/>\n",
    "### 3. How do we read the file into Pandas?\n",
    "\n",
    "We'll come back to this question. Let's first understand more about the particular structure of this JSON file so that we can decide what (if anything) to load into Pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "<br/><br/>\n",
    "\n",
    "# Digging into JSON\n",
    "\n",
    "Python has relatively good support for JSON data since it closely matches the internal python object model.  In the following cell we import the entire JSON datafile into a python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.441300Z",
     "start_time": "2018-02-02T15:15:19.358900Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(covid_file, \"rb\") as f:\n",
    "    covid_json = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `covid_json` variable is now a dictionary encoding the data in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.447238Z",
     "start_time": "2018-02-02T15:15:19.443595Z"
    }
   },
   "outputs": [],
   "source": [
    "type(covid_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine what keys are in the top level json object\n",
    "\n",
    "We can list the keys to determine what data is stored in the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.680999Z",
     "start_time": "2018-02-02T15:15:19.675696Z"
    }
   },
   "outputs": [],
   "source": [
    "covid_json.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: The JSON dictionary contains a `meta` key which likely refers to meta data (data about the data).  Meta data often maintained with the data and can be a good source of additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "We can investigate the meta data further by examining the keys associated with the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.687295Z",
     "start_time": "2018-02-02T15:15:19.682902Z"
    }
   },
   "outputs": [],
   "source": [
    "covid_json['meta'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `meta` key contains another dictionary called `view`.  This likely refers to meta-data about a particular \"view\" of some underlying database.  We will learn more about views when we study SQL later in the class.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.693946Z",
     "start_time": "2018-02-02T15:15:19.690139Z"
    }
   },
   "outputs": [],
   "source": [
    "covid_json['meta']['view'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this a nested/recursive data structure.  As we dig deeper we reveal more and more keys and the corresponding data:\n",
    "\n",
    "```\n",
    "meta\n",
    "|-> data\n",
    "    | ... (haven't explored yet)\n",
    "|-> view\n",
    "    | -> id\n",
    "    | -> name\n",
    "    | -> attribution \n",
    "    ...\n",
    "    | -> description\n",
    "    ...\n",
    "    | -> columns\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a key called description in the view sub dictionary.  This likely contains a description of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.699773Z",
     "start_time": "2018-02-02T15:15:19.695818Z"
    }
   },
   "outputs": [],
   "source": [
    "print(covid_json['meta']['view']['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Columns Metadata\n",
    "\n",
    "Another potentially useful key in the metadata dictionary is the `columns`.  This returns a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.706171Z",
     "start_time": "2018-02-02T15:15:19.701580Z"
    }
   },
   "outputs": [],
   "source": [
    "type(covid_json['meta']['view']['columns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can browse summary data in the list using python:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "1. The above meta data tells us a lot about the columns in the data including column names, potential data anomalies, and a basic statistic. \n",
    "1. JSON makes it easier (than CSV) to create **self-documented data.** \n",
    "1. Self documenting data can be helpful since it maintains its own description and these descriptions are more likely to be updated as data changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Examining the Data Field for Records\n",
    "\n",
    "We can look at a few entries in the `data` field. This is what we'll load into Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.722451Z",
     "start_time": "2018-02-02T15:15:19.717198Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(f\"{i:03} | {covid_json['data'][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/>\n",
    "\n",
    "### 3. How do we read the file into Pandas? Building a DataFrame from JSON\n",
    "\n",
    "Let's understand more about the particular structure of this JSON file so that we can decide what (if anything) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block of code we:\n",
    "1. Translate the JSON records into a dataframe:\n",
    "\n",
    "    * fields: `covid_json['meta']['view']['columns']`\n",
    "    * records: `covid_json['data']`\n",
    "    \n",
    "1. Remove columns that have no metadata description.  This would be a bad idea in general but here we remove these columns since the above analysis suggests that they are unlikely to contain useful information.\n",
    "1. Examine the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T15:15:19.793306Z",
     "start_time": "2018-02-02T15:15:19.725044Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data from JSON and assign column titles\n",
    "covid = pd.DataFrame(\n",
    "    covid_json['data'],\n",
    "    columns=[c['name'] for c in covid_json['meta']['view']['columns']])\n",
    "\n",
    "covid.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<br/><br/>\n",
    "\n",
    "## Your turn: Mauna Loa CO2 data\n",
    "\n",
    "CO2 concentrations have been monitored at Mauna Loa Observatory since 1958 ([website link](https://gml.noaa.gov/ccgg/trends/data.html)).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_file = \"data/co2_mm_mlo.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(co2_file, \"is\", os.path.getsize(co2_file) / 1e6, \"MB\")\n",
    "\n",
    "with open(co2_file, \"r\") as f:\n",
    "    print(co2_file, \"is\", sum(1 for l in f), \"lines.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### How do we read the file into Pandas?\n",
    "While we could use Python again to check out the contents, let's instead check out this file with JupyterLab (note it's a `.txt` file. Do we trust this file extension?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "---\n",
    "\n",
    "---\n",
    "<br/><br/><br/><br/><br/>\n",
    "\n",
    "Looking at the first few lines of the data, we spot some relevant characteristics:\n",
    "\n",
    "- The values are separated by white space, possibly tabs.\n",
    "- The data line up down the rows. For example, the month appears in 7th to 8th position of each line.\n",
    "- The 71st and 72nd lines in the file contain column headings split over two lines.\n",
    "\n",
    "We can useÂ `read_csv`Â to read the data into a Pandas data frame, and we provide several arguments to specify that the separators are white space, there is no header (**we will set our own column names**), and to skip the first 72 rows of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2 = pd.read_csv(\n",
    "    co2_file, header = None, skiprows = 72, \n",
    "    sep = '\\s+', # regex for continuous whitespace (next lecture)\n",
    ")\n",
    "co2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've wrangled your first set of real world data!\n",
    "\n",
    "<br/>\n",
    "\n",
    "...But our columns aren't named.\n",
    "**We need to do more EDA.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "---\n",
    "# Real World Example: Wrangling CO2 Measurements\n",
    "\n",
    "Let's continue looking at the CO2 concentrations at Mauna Loa Observatory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need column names!\n",
    "\n",
    "---\n",
    "## Exploring Variable Feature Types\n",
    "Let's go back to the raw data file to understand each feature.\n",
    "\n",
    "The NOAA [webpage](https://gml.noaa.gov/ccgg/trends/) might have some useful tidbits (in this case it doesn't).\n",
    "\n",
    "<br/><br/>\n",
    "We'll rerun `pd.read_csv`, but this time with some **custom column names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2 = pd.read_csv(\n",
    "    co2_file, header = None, skiprows = 72, \n",
    "    sep = '\\s+', # regex for continuous whitespace (next lecture)\n",
    "    names = ['Yr', 'Mo', 'DecDate', 'Avg', 'Int', 'Trend', 'Days']\n",
    ")\n",
    "co2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Let's start exploring!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scientific studies tend to have very clean data, right? Let's jump right in and make a time series plot of CO2 monthly averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x='DecDate', y='Avg', data=co2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above uses the `seaborn` plotting library (abbreviated `sns`).\n",
    "We won't cover this library in detail until next week, so focus\n",
    "on the plots themselves, not the code used to create them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes! Plotting the data uncovered a problem. It looks like we have some **missing values**. What happened here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data have unusual values like -1 and -99.99.\n",
    "\n",
    "Let's check the description at the top of the file again.\n",
    "1. -1 signifies a missing value for the number of days `Days` the equipment was in operation that month.\n",
    "1. -99.99 denotes a missing monthly average `Avg`\n",
    "\n",
    "How can we fix this? First, let's explore other aspects of our data. Understanding our data will help us decide what to do with the missing values.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Quality Checks: Reasoning about the data\n",
    "First, we consider the shape of the data. How many rows should we have?\n",
    "* If chronological order, we should have one record per month.\n",
    "* Data from March 1958 to August 2019.\n",
    "* We should have $ 12 \\times (2019-1957) - 2 - 4 = 738 $ records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Nice!! The number of rows (i.e. records) match our expectations.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "Let's now check the quality of each feature.\n",
    "\n",
    "## Understanding Missing Value 1: `Days`\n",
    "`Days` is a time field, so let's analyze other time fields to see if there is an explanation for missing values of days of operation.\n",
    "\n",
    "Let's start with **months** `Mo`.\n",
    "\n",
    "Are we missing any records? The number of months should have 62 or 61 instances (March 1957-August 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2[\"Mo\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected Jan, Feb, Sep, Oct, Nov, and Dec have 61 occurrences and the rest 62.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "Next let's explore **days** `Days` itself, which is the number of days that the measurement equipment worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(co2['Days']);\n",
    "plt.title(\"Distribution of days feature\")\n",
    "plt.show() # suppresses unneeded plotting output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of data quality, a handful of months have averages based on measurements taken on fewer than half the days. In addition, there are nearly 200 missing values--**that's about 27% of the data**!\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "Finally, let's check the last time feature, **year** `Yr`.\n",
    "\n",
    "Let's check to see if there is any connection between missingness and the year of the recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"Yr\", y=\"Days\", data=co2);\n",
    "plt.title(\"Day field by Year\"); # the ; suppresses output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "\n",
    "* All of the missing data are in the early years of operation.\n",
    "* It appears there may have been problems with equipment in the mid to late 80s.\n",
    "\n",
    "**Potential Next Steps**:\n",
    "* Confirm these explanations through documentation about the historical readings.\n",
    "* Maybe drop earliest recordings? However, we would want to delay such action until after we have examined the time trends and assess whether there are any potential problems.\n",
    "\n",
    "---\n",
    "<br/><br/>\n",
    "\n",
    "## Understanding Missing Value 2: `Avg`\n",
    "Next, let's return to the -99.99 values in `Avg` to analyze the overall quality of the CO2 measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms of average CO2 measurements\n",
    "sns.displot(co2['Avg']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-missing values are in the 300-400 range (a regular range of CO2 levels).\n",
    "\n",
    "We also see that there are only a few missing `Avg` values (**<1% of values**). Let's examine all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2[co2[\"Avg\"] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be a pattern to these values, other than that most records also were missing `Days` data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop or Impute Missing `Avg` Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we want to fix the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x='DecDate', y='Avg', data=co2)\n",
    "plt.title(\"CO2 Average By Month\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are plotting `Avg` vs `DecDate`, we should just focus on dealing with missing values for `Avg`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a few options:\n",
    "1. Drop those records\n",
    "1. Replace -99.99 with NaN\n",
    "1. Substitute it with a likely value for the average CO2?\n",
    "\n",
    "What do you think are the pros and cons of each possible action?\n",
    "\n",
    "---\n",
    "<br/><br/>\n",
    "Let's examine each of these three options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop missing values\n",
    "co2_drop = co2[co2['Avg'] > 0]\n",
    "\n",
    "# 2. Replace NaN with -99.99\n",
    "co2_NA = co2.replace(-99.99, np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also use a third version of the data.\n",
    "First, we note that the dataset already comes with a **substitute value** for the -99.99.\n",
    "\n",
    "From the file description:\n",
    "\n",
    ">  The `interpolated` column includes average values from the preceding column (`average`)\n",
    "and **interpolated values** where data are missing.  Interpolated values are\n",
    "computed in two steps...\n",
    "\n",
    "The `Int` feature has values that exactly match those in `Avg`, except when `Avg` is -99.99, and then a **reasonable** estimate is used instead.\n",
    "So, the third version of our data will use the `Int` feature instead of `Avg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Use interpolated column which estimates missing Avg values\n",
    "co2_impute = co2.copy()\n",
    "co2_impute['Avg'] = co2['Int']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "What's a **reasonable** estimate?\n",
    "\n",
    "To answer this question, let's zoom in on a short time period, say the measurements in 1958 (where we know we have two missing values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results of plotting data in 1958\n",
    "\n",
    "def line_and_points(data, ax, title):\n",
    "    # assumes single year, hence Mo\n",
    "    ax.plot('Mo', 'Avg', data=data)\n",
    "    ax.scatter('Mo', 'Avg', data=data)\n",
    "    ax.set_xlim(2, 13)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(np.arange(3, 13))\n",
    "\n",
    "def data_year(data, year):\n",
    "    return data[data[\"Yr\"] == 1958]\n",
    "    \n",
    "# uses matplotlib subplots\n",
    "# you may see more next week; focus on output for now\n",
    "fig, axes = plt.subplots(ncols = 3, figsize=(12, 4), sharey=True)\n",
    "\n",
    "year = 1958\n",
    "line_and_points(data_year(co2_drop, year), axes[0], title=\"1. Drop Missing\")\n",
    "line_and_points(data_year(co2_NA, year), axes[1], title=\"2. Missing Set to NaN\")\n",
    "line_and_points(data_year(co2_impute, year), axes[2], title=\"3. Missing Interpolated\")\n",
    "\n",
    "fig.suptitle(f\"Monthly Averages for {year}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the big picture since there are only 7 `Avg` values missing (**<1%** of 738 months), any of these approaches would work.\n",
    "\n",
    "However there is some appeal to **option 3: Imputing**:\n",
    "* Shows seasonal trends for CO2\n",
    "* We are plotting all months in our data as a line plot\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "Let's replot our original figure with option 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x='DecDate', y='Avg', data=co2_impute)\n",
    "plt.title(\"CO2 Average By Month, Imputed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty close to what we see on the NOAA [website](https://gml.noaa.gov/ccgg/trends/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presenting the data: A Discussion on Data Granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the description:\n",
    "* monthly measurements are averages of average day measurements.\n",
    "* The NOAA GML website has datasets for daily/hourly measurements too.\n",
    "\n",
    "The data you present depends on your research question.\n",
    "\n",
    "**How do CO2 levels vary by season?**\n",
    "* You might want to keep average monthly data.\n",
    "\n",
    "**Are CO2 levels rising over the past 50+ years, consistent with global warming predictions?**\n",
    "* You might be happier with a **coarser granularity** of average year data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_year = co2_impute.groupby('Yr').mean()\n",
    "sns.lineplot(x='Yr', y='Avg', data=co2_year)\n",
    "plt.title(\"CO2 Average By Year\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we see a rise by nearly 100 ppm of CO2 since Mauna Loa began recording in 1958."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
